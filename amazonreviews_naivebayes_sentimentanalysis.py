# -*- coding: utf-8 -*-
"""AmazonReviews_NaiveBayes_SentimentAnalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vmIgkAwgV_s2zGYozXEsluMH47CpBUeN
"""

import nltk
import re
import numpy as np
import pandas as pd
#Tokenization of text
from nltk.tokenize import word_tokenize,sent_tokenize
#remove stop-words
from nltk.corpus import stopwords # library 
nltk.download('stopwords')
all_stopwords = set(stopwords.words('english')) # set the language 
from typing import List

# reading review data with panda frames 
reviews_data=pd.read_csv('Firstproductdata.csv')
reviews_data.describe()

#sentiment counts
reviews_data['sentiment'].value_counts()

def preprocess_text(text: str) -> List[str]:
    # Looking at the text we see that <br></br> which is HTML tag for line break can be a good splitter
    # A sentence (atleast well structured) often has a full spot at the end. We use these two for word breaks
    pattern1 = re.compile("<br /><br />|\.")
    lines = re.split(pattern1, text)
    # you can break a sentence into words using whitespace based split
    tokens = []
    for line in lines:
        tokens += line.split(" ")

    # lowercase and remove any non-alphanumeric characters from tokens for normalize
    normalized_tokens = [re.sub(r"\W+", "", token.lower()) for token in tokens]
    return  " ".join([
            token
            for token in normalized_tokens
            if token and token not in all_stopwords and len(tokens) > 1 
        ])

#apply preprocessing to review data
reviews_data['reviews_text'] = reviews_data['reviews_text'].apply(preprocess_text)

!pip install imbalanced-learn==0.6.2

#split the dataset  
#train dataset
#train_reviews=reviews_data.reviews_text[:6452]
#train_sentiments=reviews_data.sentiment[:6452]
#test dataset
#test_reviews=reviews_data.reviews_text[6452:9624]
#test_sentiments=reviews_data.sentiment[6452:9624]
#validation (blind) dataset
#blind_reviews=reviews_data.reviews_text[9624:]
#blind_sentiments=reviews_data.sentiment[9624:]
#print(train_reviews.shape,train_sentiments.shape)
#print(test_reviews.shape,test_sentiments.shape)
#print(blind_reviews.shape,blind_sentiments.shape)

#OVERSAMPLING of Data
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import RandomOverSampler
from imblearn import FunctionSampler

train_reviews, test_reviews, train_sentiments, test_sentiments = train_test_split(reviews_data['reviews_text'],
                                  reviews_data['sentiment'], test_size=0.3, random_state=0)

def resample(X, y):
    return RandomOverSampler().fit_resample(X, y)

sampler = FunctionSampler(func=resample, validate=False)
#reshape and ravel coverts pandas df to numpy array, since RandomOverSampler only accepts numpy array
train_reviews, train_sentiments = sampler.fit_resample(train_reviews.values.reshape(-1,1), train_sentiments.ravel())

train_reviews = train_reviews.reshape(train_reviews.size,)

train_reviews = pd.Series(train_reviews)
train_sentiments = pd.Series(train_sentiments)
train_sentiments.value_counts()

# CountVectorizer implements both tokenization and occurrence counting in a single class. Read more here https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
# You can also reuse the from scratch code we learnt in previous class
# TfidfVectorizer Convert a collection of raw documents to a matrix of TF-IDF features. Equivalent to CountVectorizer followed by TfidfTransformer.
# from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
#Count vectorizer with 
lower_count_thr = 50 # rare words/tokens
upper_count_thr = 200 # frequent/common tokens

tv=TfidfVectorizer(min_df=lower_count_thr,max_df=upper_count_thr,binary=False,ngram_range=(1,1))
#transformed train reviews
tv_train_reviews=tv.fit_transform(train_reviews)
#transformed test reviews
tv_test_reviews=tv.transform(test_reviews)

#transformed validation reviews
#tv_blind_reviews=tv.transform(blind_reviews)
#tv_blind_reviews.head()
print('BOW_cv_train:',tv_train_reviews.shape)
print('BOW_cv_test:',tv_test_reviews.shape)
#print('BOW_cv_blind:',tv_blind_reviews.shape)

#Now generate binary (true, false) labels from sentiment values. positive maps to 1, negative maps to 0
#from sklearn.preprocessing import LabelBinarizer
#lb=LabelBinarizer()
#transformed sentiment data
#sentiment_data=lb.fit_transform(reviews_data['sentiment'])
#print(sentiment_data.shape)

#Spliting the sentiment data
#train_sentiments=sentiment_data[:6452]
#test_sentiments=sentiment_data[6452:9624]
#blind_sentiments=sentiment_data[9624:]
#print(train_sentiments.shape)
#print(test_sentiments.shape)
#print(blind_sentiments.shape)

from sklearn.naive_bayes import MultinomialNB
model = MultinomialNB()
model.fit(tv_train_reviews,train_sentiments)
labels = model.predict(tv_test_reviews)

from sklearn.metrics import accuracy_score
acc = accuracy_score(test_sentiments,labels, normalize=True) * float(100)
print('\n****Test accuracy is',(acc))

from sklearn.metrics import confusion_matrix
cm_test = confusion_matrix(test_sentiments,labels)
cm_test

import seaborn as sns
sns.heatmap(cm_test,annot=True,fmt='d')

model_tfidf=model.fit(tv_train_reviews,train_sentiments)
print(model_tfidf)

model_tfidf_predict_test=model.predict(tv_test_reviews)
print(model_tfidf_predict_test)

from sklearn.metrics import classification_report,confusion_matrix,accuracy_score
model_tfidf_score=accuracy_score(test_sentiments,model_tfidf_predict_test)
print("model_tfidf_score :",model_tfidf_score)

#Classification report for tfidf features
model_tfidf_report_test=classification_report(test_sentiments,model_tfidf_predict_test,target_names=['positive','negative'])
print(model_tfidf_report_test)