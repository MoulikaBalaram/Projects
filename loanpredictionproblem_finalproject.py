# -*- coding: utf-8 -*-
"""LoanPredictionProblem_FinalProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zDSpx6ItKkf1e5kqUE9B8ULa6kv7-7Z6

# **FINAL PROJECT - SPECIAL TOPICS IN BUSINESS ANALYTICS**
#**LOAN PREDICTION PROBLEM**


---

Dream Housing Finance company deals in all home loans with their presence across all urban, semi urban and rural areas. The customer first applies for home loan, then the company validates the customer's eligibility to grant a loan.

Company wants to automate the loan eligibility process (real time) based on customer detail provided while filling online application form. These details are Gender, Marital Status, Education, Number of Dependents, Income, Loan Amount, Credit History and others. To automate this process, they have given a problem to identify the customers segments, those are eligible for loan amount so that they can specifically target these customers.

DATA DESCRIPTION
The dataset consists of 13 different factors affecting the customers' Loan Status.
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

"""###IMPORTING REQUIRED LIBRARIES"""

# Commented out IPython magic to ensure Python compatibility.
# importing the required libraries
import pandas as pd
import numpy as np
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
# %matplotlib inline

# check version on sklearn
print('Version of sklearn:', sklearn.__version__)

"""###LOADING THE DATA SET"""

# loading the pre-processed dataset
data = pd.read_csv('loan_prediction_data.csv')

# looking at the first five rows of the dataset
data.head()

# checking missing values
data.isnull().sum()

# checking the data type
data.dtypes

# removing the loan_ID since these are just the unique values
data = data.drop('Loan_ID', axis=1)

# looking at the shape of the data
data.shape

# separating the independent and dependent variables

# storing all the independent variables as X
X = data.drop('Loan_Status', axis=1)

# storing the dependent variable as y
y = data['Loan_Status']

X.head()

# shape of independent and dependent variables
X.shape, y.shape

"""###EXPLORATORY ANALYSIS"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
pd.crosstab(data.Gender,data.Loan_Status).plot(kind='bar')
plt.title('Loan Frequency for Gender Title')
plt.xlabel('Gender')
plt.ylabel('Frequency of Loan status')

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
pd.crosstab(data.Married,data.Loan_Status).plot(kind='bar')
plt.title('Loan Frequency for Married Title')
plt.xlabel('Married')
plt.ylabel('Frequency of Loan status')

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
pd.crosstab(data.Dependents,data.Loan_Status).plot(kind='bar')
plt.title('Loan Frequency for Applicant Title')
plt.xlabel('Gender')
plt.ylabel('Frequency of Loan status')

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
pd.crosstab(data.Loan_Amount_Term,data.Loan_Status).plot(kind='bar')
plt.title('Loan Frequency for Loan Amount term Title')
plt.xlabel('Loan_Amount_Term')
plt.ylabel('Frequency of Loan status')

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
pd.crosstab(data.Loan_Amount_Term,data.Loan_Status).plot(kind='bar')
plt.title('Loan Frequency for Credit History Title')
plt.xlabel('Credit_History')
plt.ylabel('Frequency of Loan status')

"""###TRAINING AND VALIDATION SETS"""

# Creating training and validation set

# stratify will make sure that the distribution of classes in train and validation set it similar
# random state to regenerate the same train and validation set
# test size 0.2 will keep 20% data in validation and remaining 80% in train set

X_train,X_test,y_train,y_test = train_test_split(X,y,stratify=data['Loan_Status'],random_state=10,test_size=0.2)

# shape of training and validation set
(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)

"""##LOGISTIC REGRESSION"""

import statsmodels.api as sm
import numpy as np
np.array(X).astype(np.float)
logit_model=sm.Logit(y,X)
result=logit_model.fit()
print(result.summary2())

from sklearn.linear_model import LogisticRegression
from sklearn import metrics
logreg = LogisticRegression()
logreg.fit(X_train, y_train)

y_pred = logreg.predict(X_test)
print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
logit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))
fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])
plt.figure()
plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.savefig('Log_ROC')
plt.show()

"""## NEURAL NETWORK

### DEPENDENCIES FOR NEURAL NETWORK
"""

# checking the version of keras
import keras
print(keras.__version__)

# checking the version of tensorflow
import tensorflow as tf
print(tf.__version__)

# importing the sequential model
from keras.models import Sequential

# importing different layers from keras
from keras.layers import InputLayer, Dense

# number of input neurons
X_train.shape

# number of features in the data
X_train.shape[1]

# defining input neurons
input_neurons = X_train.shape[1]

# define number of output neurons
output_neurons = 1

# define hidden layers and neuron in each layer
number_of_hidden_layers = 2
neuron_hidden_layer_1 = 10
neuron_hidden_layer_2 = 5

# defining the architecture of the model
model = Sequential()
model.add(InputLayer(input_shape=(input_neurons,)))
model.add(Dense(units=neuron_hidden_layer_1, activation='relu'))
model.add(Dense(units=neuron_hidden_layer_2, activation='relu'))
model.add(Dense(units=output_neurons, activation='sigmoid'))

# summary of the model
model.summary()

# number of parameters between input and first hidden layer

input_neurons*neuron_hidden_layer_1

# number of parameters between input and first hidden layer

# adding the bias for each neuron of first hidden layer

input_neurons*neuron_hidden_layer_1 + 10

# number of parameters between first and second hidden layer

neuron_hidden_layer_1*neuron_hidden_layer_2 + 5

# number of parameters between second hidden and output layer

neuron_hidden_layer_2*output_neurons + 1

# compiling the model

# loss as binary_crossentropy, since we have binary classification problem
# defining the optimizer as adam
# Evaluation metric as accuracy

model.compile(loss='binary_crossentropy',optimizer='Adam',metrics=['accuracy'])

# training the model

# passing the independent and dependent features for training set for training the model

# validation data will be evaluated at the end of each epoch

# setting the epochs as 50

# storing the trained model in model_history variable which will be used to visualize the training process

model_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50)

# getting predictions for the validation set
prediction = model.predict_classes(X_test)

# calculating the accuracy on validation set
accuracy_score(y_test, prediction)

# summarize history for loss
plt.plot(model_history.history['loss'])
plt.plot(model_history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

# summarize history for accuracy
plt.plot(model_history.history['accuracy'])
plt.plot(model_history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

"""## RANDOM FOREST CLASSIFIER"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
X, y = make_classification(n_samples=1000, n_features=11,
                           n_informative=2, n_redundant=0,
                            random_state=0, shuffle=False)
clf = RandomForestClassifier(max_depth=2, random_state=0)
clf.fit(X, y)
RandomForestClassifier(...)
# Use the forest's predict method on the test data
predictions = clf.predict(X_test)
from sklearn import metrics
# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

"""## Comparing Accuracy Values of 3 Algorithms

> **Logistic Regression** - 81 %

> **Neural Network** - 81.3 %

> **Random Forest** - 81.3 %

By comparing the three algorithms, Neural Network and Random Forest could be used as they yield an accuracy of 81.3 % each and they both are recommended to be best models for the dataset.
"""